{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Dependencies form the tensorflow dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hyperparameters\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Import data into tensorflow dataset object form the folders saved in local PC\n",
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"d:\\Documents\\My_Projects\\DeepLearning\\potatoDiseaseClassification_Project3\",\n",
    "    shuffle=True, \n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),  # 256 * 256 size\n",
    "    batch_size= BATCH_SIZE\n",
    ")\n",
    "#The datasets are in three different folders/classes : healthy leaves, diseased leaves with Early Blight, Late Blight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Names of classes are read in digit format. Finding their class names\n",
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256, 256, 3)\n",
      "[1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 2 1 1 0 0 1 1 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.numpy())\n",
    "\n",
    "#Each element in the dataset is a tuple. First element is a batch of 32 elements of images. Second element is a batch of 32 elements of class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the visulisation cell will shut down the kernel, So prevent doing so.\n",
    "This maybe because of loading the high constrast, high defination images that cell is not able to handel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the images from the dataset\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for image_batch, labels_batch in dataset.take(1):\n",
    "#     for i in range(12):\n",
    "#         ax = plt.subplot(3, 4, i + 1)\n",
    "#         plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "#         plt.title(class_names[labels_batch[i]])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now, we write the function to split the dataset into 3 types\n",
    "\n",
    "Training: Dataset to be used while training --> 80%\n",
    "Validation: Dataset to be tested against while training --> 10%\n",
    "Test: Dataset to be tested against after we trained a model --> 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset) #We have 32 batches and each batch has 68 images : 38 * 68 (to accomodate all images in the batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.400000000000006"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "len(dataset)*train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = dataset.take(54)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = dataset.skip(54)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.800000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size=0.1\n",
    "len(dataset)*val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = test_ds.take(6)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = test_ds.skip(6)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the concepts from the above, we define the function\n",
    "\n",
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache, Shuffle, and Prefetch the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moreover, to improve model performance, we should normalize the image pixel value (keeping them in range 0 and 1 by dividing by 256).\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    #Resize the images if not in the dataset to 256 * 256 during prediction \n",
    "    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE), #256 * 256\n",
    "    # Scale the image to 255\n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the image variations from the single image ->  Change in constrast, position of image, flip the image, zoom.\n",
    "This is done so that when we are training we train all variations of the single image we have better prediction.\n",
    "\n",
    "Data Augmentation is needed when we have less data, this boosts the accuracy of our model by augmenting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model : Model Architecture\n",
    "\n",
    "We use a CNN coupled with a Softmax activation in the output layer. We also add the initial layers for resizing, normalization and Data Augmentation.\n",
    "\n",
    "We are going to use convolutional neural network (CNN) which is popular for image classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (32, 256, 256, 3)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (32, 254, 254, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (32, 127, 127, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (32, 125, 125, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (32, 62, 62, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (32, 60, 60, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (32, 30, 30, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (32, 28, 28, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (32, 14, 14, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (32, 12, 12, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (32, 6, 6, 64)           0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (32, 4, 4, 64)            36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (32, 2, 2, 64)           0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (32, 256)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (32, 64)                  16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 3)                   195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,747\n",
      "Trainable params: 183,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the Model : \n",
    "We use adam Optimizer, SparseCategoricalCrossentropy for losses, accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 96s 2s/step - loss: 0.3909 - accuracy: 0.8484 - val_loss: 0.6671 - val_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 102s 2s/step - loss: 0.3372 - accuracy: 0.8628 - val_loss: 0.1668 - val_accuracy: 0.9271\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 93s 2s/step - loss: 0.2054 - accuracy: 0.9288 - val_loss: 0.0884 - val_accuracy: 0.9740\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 69s 1s/step - loss: 0.2007 - accuracy: 0.9288 - val_loss: 0.1150 - val_accuracy: 0.9635\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 79s 1s/step - loss: 0.1414 - accuracy: 0.9514 - val_loss: 0.0861 - val_accuracy: 0.9583\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 73s 1s/step - loss: 0.1110 - accuracy: 0.9595 - val_loss: 0.0900 - val_accuracy: 0.9896\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 70s 1s/step - loss: 0.0757 - accuracy: 0.9734 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 66s 1s/step - loss: 0.0773 - accuracy: 0.9699 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 63s 1s/step - loss: 0.0470 - accuracy: 0.9797 - val_loss: 0.0614 - val_accuracy: 0.9792\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 93s 2s/step - loss: 0.0619 - accuracy: 0.9803 - val_loss: 0.0267 - val_accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history\n",
    "\n",
    "history.params\n",
    "\n",
    "history.history.keys()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
    "# plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
    "# plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now Run prediction on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First image to predict\n",
      "Actual Label:  Potato___Early_blight\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "predicted label:  Potato___Early_blight\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for images_batch, labels_batch in test_ds.take(1):\n",
    "    first_image = image_batch[0].numpy().astype('uint8')\n",
    "    first_label = labels_batch[0].numpy()\n",
    "\n",
    "    print(\"First image to predict\")\n",
    "    first_image\n",
    "    #The actual label of the image\n",
    "    print(\"Actual Label: \", class_names[first_label])\n",
    "\n",
    "    #To predict the batch which contains 32 images\n",
    "    batch_prediction = model.predict(images_batch)\n",
    "    #To predict the batch prediction of the first image from that batch\n",
    "    print(\"predicted label: \", class_names[np.argmax(batch_prediction[0])])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below function is for the inference. We will try to run the inference on few sample images in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 410ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "# plt.figure(flgsize=(15,15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9): # Here we take only 9 images to test\n",
    "        # ax = plt.subplot(3, 3, i+1)\n",
    "        predicted_class, confidence = predict(model, images[i].numpy)\n",
    "        #Actual class of that specific image\n",
    "        actual_class = class_names[labels[i]]\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save The Model \n",
    "We will add the model to the list of models as a new version every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/1\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model in the folder models \n",
    "# One can save the multiple version of the model also\n",
    "\n",
    "model_version = 1\n",
    "model.save(f\"./models/{model_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model_versions/odels/3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model_versions/odels/3\\assets\n"
     ]
    }
   ],
   "source": [
    "# If we want to have multiple models and want to increment the model version automatically\n",
    "# We just import the os library and auto increment the model number in the models folder.\n",
    "\n",
    "import os\n",
    "model_version=max([int(i) for i in os.listdir(\"./saved_model_versions/\") + [0]])+1\n",
    "model.save(f\"./saved_model_versions/odels/{model_version}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
